{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nltk, re\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, numpy, gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Text selection</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to randomly select a fraction of the corpus to train the topic model on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of this process is the list of file names in alphabetical order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'intro_dh_projekt/Dream_All_Texts_Plain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(os.listdir(dirname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unknown pubDate - unknown author -  -                        -  1665.xml.txt',\n",
       " 'unknown pubDate - unknown author -  L        -  1679.xml.txt',\n",
       " 'unknown pubDate - unknown author -  L     L              -  1687.xml.txt',\n",
       " 'unknown pubDate - unknown author -  L  L  1 1679 -  1679.xml.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[34773:34777]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let the function `randrange()` create the indices of the files to be picked. Using a set ensures that no file will be selected twice. The size of the set is such that a tenth of the corpus will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import random, randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = set()\n",
    "while len(nums) < 3477:\n",
    "    nums.add(randrange(34777))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_sorted = sorted(list(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3477"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nums_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First five texts to select are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 20, 29, 36]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading in the data</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the indices ready and sorted, we iterate ove them, using each to open the respective file in the directory, tokenize it, and append it to the initially empty list `texts`. The result will be a list of lists of words (as well as punctuation, numbers etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for num in nums_sorted:\n",
    "    filename = filenames[num]\n",
    "    with open(os.path.join(dirname, filename)) as text:\n",
    "        texts.append(word_tokenize(text.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3477"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " '?',\n",
       " 'xml',\n",
       " 'version=',\n",
       " \"''\",\n",
       " '1.0',\n",
       " \"''\",\n",
       " 'encoding=',\n",
       " \"''\",\n",
       " 'UTF-8',\n",
       " \"''\",\n",
       " '?',\n",
       " '>',\n",
       " 'The',\n",
       " 'Knavish',\n",
       " 'MERCHANT',\n",
       " '(',\n",
       " 'Now',\n",
       " 'turn',\n",
       " \"'d\",\n",
       " 'Warehouseman',\n",
       " ')',\n",
       " 'CHARACTARIZED',\n",
       " 'OR',\n",
       " 'A',\n",
       " 'severe',\n",
       " 'Scourge',\n",
       " ',',\n",
       " 'for',\n",
       " 'an']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform all the words in the corpus to lowercase and clear the corpus of numbers and punctuation except for full stops, as these will be needed for the chunking. It would have been an option to also leave in questions marks and exclamation marks, but the full stop approach seemed to suffice for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clear = [[w.lower() for w in text if w.isalpha() or w == '.'] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3477"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xml',\n",
       " 'the',\n",
       " 'knavish',\n",
       " 'merchant',\n",
       " 'now',\n",
       " 'turn',\n",
       " 'warehouseman',\n",
       " 'charactarized',\n",
       " 'or',\n",
       " 'a',\n",
       " 'severe',\n",
       " 'scourge',\n",
       " 'for',\n",
       " 'an',\n",
       " 'unjust',\n",
       " 'cruel',\n",
       " 'and',\n",
       " 'unconscionable',\n",
       " 'adversary',\n",
       " 'by',\n",
       " 'philadelphus',\n",
       " 'verax',\n",
       " 'a',\n",
       " 'cordial',\n",
       " 'friend',\n",
       " 'to',\n",
       " 'his',\n",
       " 'honest',\n",
       " 'though',\n",
       " 'injuriously']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_clear[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auxiliary dictionary notes down the year of origin of each file used in the corpus by extracting the first occurence of a four-digit sequence in the title. This method is certainly not perfect but presumably good enough given the size of the corpus and the format of the titles. If no year is found, the entry will be None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "getyear = {}\n",
    "for num in nums_sorted:\n",
    "    filename = filenames[num]\n",
    "    years = re.findall('[0-9]{4}', filename)\n",
    "    year = next((y for y in years), None)\n",
    "    getyear[num] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3477"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(getyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              1661 -   -                           - .xml.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[nums_sorted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1661'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getyear[nums_sorted[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Chunking</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the chunking function. This function takes as input a word-tokenized text, that is, a list of words and full stops. With `n` being the chunk size and `i` the starting position (initially 0), it will jump to the end position of the desired chunk (`n-1`) and check whether or not this list element is a full stop. `n` will then be incremented until a full stop is found, and the slice with end position `n` exclusive will be added to the initially empty list. The start position of the next chunk will be `i+n`, and the chunk size will be reset to its initial value. This goes on so long as the calculated end position is within the range of the text.\n",
    "The last chunk, which is bound to be shorter than the desired chunk size, will be added to the chunk list directly or, if it is shorter than a predetermined minimum size, to the last chunk in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkthis(txt, chunksize, minsize):\n",
    "    texts = []\n",
    "    n = chunksize\n",
    "    i = 0\n",
    "    while i+n <= len(txt) and n != 0:\n",
    "        while txt[i+n-1] != '.' and i+n<len(txt):\n",
    "            n+=1\n",
    "        chunk = txt[i:i+n]\n",
    "        texts.append(chunk)\n",
    "        i = i+n\n",
    "        n = chunksize\n",
    "    #if last chunk is shorter than minsize, append to last txt in chunked array (if there is one)\n",
    "    if len(txt) - i < minsize and len(texts)>0:\n",
    "        texts[-1] += (txt[i:])\n",
    "    #if last chunk is anywhere between 100 and desired chunksize, append directly\n",
    "    else:\n",
    "        texts.append(txt[i:])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this chunking function, we can iterate over the cleared corpus and do the following: First extract the file index of the text from `nums_sorted` (the random numbers list), then chunk the text, resulting in a list of lists of approximately 400 words each, add these chunks to an initially empty list `texts_chunked`, and add the file index to a separate list `chunk_index` according to the number of chunks created. Thus, if a text was split into 100 chunks, the file index of that text will be noted down 100 times, so that calling the list index of the chunk will return the index of the file it was extracted from. This will be important to get the year of origin of the chunks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file index: 11\n",
      "5 chunks\n",
      "file index: 12\n",
      "67 chunks\n",
      "file index: 20\n",
      "437 chunks\n",
      "file index: 29\n",
      "5 chunks\n",
      "file index: 36\n",
      "107 chunks\n"
     ]
    }
   ],
   "source": [
    "chunk_index = []\n",
    "texts_chunked = []\n",
    "for i in range(len(texts_clear)):\n",
    "    file_index = nums_sorted[i] #let's assume first random number is 11, therefore file_index in first loop is 11   \n",
    "    chunks = chunkthis(texts_clear[i], 400, 100) #let's assume this creates 5 chunks\n",
    "    texts_chunked += chunks #add chunks to chunk list\n",
    "    chunk_index += [file_index]*len(chunks) #add file index 11 5 times --> chunk_index[4] will then return file index of 4th (0-based) chunk\n",
    "    if i < 5:\n",
    "        print('file index:', file_index)\n",
    "        print(len(chunks), 'chunks')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200577"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "410\n",
      "429\n",
      "412\n",
      "442\n",
      "412\n",
      "412\n",
      "403\n",
      "404\n",
      "429\n"
     ]
    }
   ],
   "source": [
    "for txt in texts_chunked[:10]:\n",
    "    print(len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Removal of stop words</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clear the attained chunks of stop words and full stops, so that only words not appearing among the 500 most frequent terms remain. Since the stop word list contains punctuation and numbers, we end up removing 475 different terms. 'XML' was added to the stop word list as each file starts with an XML declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some spagetti code to read in the stop words\n",
    "stop_words=[]\n",
    "i = 0\n",
    "with open('stopwords.txt', 'rb') as f:\n",
    "    while i < 500:\n",
    "        stop_words.append(str(f.readline()).split()[0][2:])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'of',\n",
       " 'and',\n",
       " 'to',\n",
       " 'in',\n",
       " 'that',\n",
       " 'a',\n",
       " 'is',\n",
       " 'it',\n",
       " 'for',\n",
       " 'his',\n",
       " 'as',\n",
       " 'be',\n",
       " 'he',\n",
       " 'not',\n",
       " 'by',\n",
       " 'but',\n",
       " 'they',\n",
       " 'which',\n",
       " 'with']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [w for w in stop_words if w.isalpha()] #remove numbers and punctuation from stop words\n",
    "stop_words.append('xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_final = [[w for w in text if w.isalpha() and w not in stop_words] for text in texts_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200577"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting corpus `texts_final` now contains 200577 text chunks extracted from 3477 texts in word-tokenized form, cleared of stop words and punctuation. As we can see by comparing the lengths of the chunks before and after weeding out the stop words, the chunks have shrunk considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "134\n",
      "137\n",
      "140\n",
      "141\n",
      "165\n",
      "155\n",
      "135\n",
      "170\n",
      "252\n"
     ]
    }
   ],
   "source": [
    "for txt in texts_final[:10]:\n",
    "    print(len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Saving data</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle, we save the relevant data structures in a compressed format: the final corpus, the chunk index allowing us to retrace what text a chunk was extracted from, and the file indeces of the used texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(texts_final, open(\"texts_final_new.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(chunk_index, open(\"chunk_index.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nums_sorted, open(\"file_index.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = pickle.load(open(\"texts_final_400w.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Preparing the data for Topan</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step creates a two-dimensional array with each row corresponding to a text chunk. The two columns represent chunk identifier (a combination of file index and the number of the chunk extracted from that file, like so: `11:1`, `11:2`, ...) and the words of that chunk in simple string format. Saving this table as a CSV file allowed us to read the texts into Topan as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anarray = []\n",
    "fileindex_prev = 0\n",
    "n=0\n",
    "for i in range(len(texts_final)):\n",
    "    fileindex = chunk_index[i]\n",
    "    if fileindex != fileindex_prev:\n",
    "        n = 1\n",
    "    else:\n",
    "        n+=1\n",
    "    chunkindex = str(fileindex) + ':' +str (n)\n",
    "    astring = \"\"\n",
    "    for word in texts_final[i]:\n",
    "        astring = astring + word + \" \"\n",
    "    anarray.append([chunkindex, astring])\n",
    "    fileindex_prev = fileindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt(\"all_the_chunks.csv\", anarray, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Topic model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual LDA topic modelling. We use Python's gensim library to create a dictionary mapping each remaining term in the corpus to a unique id. The standard format is id2token but the class Dictionary provides the function token2id as well. This `dictionary` is then used to covert the corpus to a numeric format on the basis of the bag-of-words assumption. Each text is thus treated as a bag of words, where only word frequencies matter but word sequence is ignored. As a result, each text in the corpus is represented as a list of (word id, word frequency) tuples. The resulting data structure of `corpus` is therefore a list of lists of numeric tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_reduced = [corpus[i] for i in range(0, len(corpus), 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA was chosen as a model since it works relatively autonomously, that is, the relevant parameters alpha (affecting per-document topic distribution) and beta (affecting per-topic word distribution) are learned from the corpus during training. The main difficulty was in determining the optimal number of topics. This was mitigated by sheer lack of computing power though, so that we ended up producing a relatively small number of topics. On top of that, the training data for the topic model had to be further reduced to one fifth of the generated corpus, thus using 40116 text chunks instead of 200577. Since the resulting topics appeared meaningful enough to us, we left it at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm29 = gensim.models.LdaModel(corpus_reduced, id2word=dictionary, num_topics=29, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm59 = gensim.models.LdaModel(corpus_reduced, id2word=dictionary, num_topics=59, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm29.save('tm29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm59.save('tm59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm29 = gensim.models.LdaModel.load('tm29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step was to visualize the topic model in application to the corpus using Python's pyLDAvis package. Although many of the smaller topics are crammed into one corner of the coordinate system, we can see that each quadrant contains a number of topics and the bigger ones are not extremely overlapping, which suggests that topic coherence might be reasonably good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = gensimvis.prepare(tm29, corpus_reduced, dictionary, sort_topics=False)\n",
    "pyLDAvis.show(lda_display)\n",
    "pyLDAvis.save_html(lda_display, 'tm29_reduced.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
